{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "- What are eigenvectors and eigenvalues?\n",
    "- If you don't have an issue with dimensionality, does PCA improve scores?\n",
    "- Why the covariance matrix? \n",
    "    - The first vector explains the covariance the most and therefore, projecting onto it, maintain the most information in regards to covariance. \n",
    "    - Covariance is square, it's symmetrical\n",
    "        - our eigenvalues get upgraded\n",
    "\n",
    "- How do you interpret a model when you use PCA? - You can't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "- Define Eigenvalues and Eigenvectors\n",
    "- Describe how these are used in PCA\n",
    "- Apply PCA to reduce dimensions of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "- Questions\n",
    "- Explain eigenvalues and eigenvectors and why they're awesome\n",
    "- Apply eigen decomposition to the correlation matrix and discuss how it's used in PCA\n",
    "- Apply PCA to some dataset that we create using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues and Eigenvectors are only applied on square matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13, 11, 17],\n",
       "       [13, 11, 15],\n",
       "       [15, 19, 13]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.randint(10, 20, size=(3, 3))\n",
    "A "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigen Properties\n",
    "- Eigen Values multiply to the determinent of A\n",
    "- Eigen Values add to the trace of matrix A (trace = sum of diagonals)\n",
    "- Eigen Vectors are orthonormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals, evecs = np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42.46064274,  0.        ,  0.        ],\n",
       "       [ 0.        , -0.834997  ,  0.        ],\n",
       "       [ 0.        ,  0.        , -4.62564573]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals_diag = np.diag(evals)\n",
    "evals_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([42.46064274, -0.834997  , -4.62564573]),\n",
       " array([[-0.56327554, -0.82567166, -0.58550079],\n",
       "        [-0.53355856,  0.30738993, -0.25241239],\n",
       "        [-0.63090089,  0.47305152,  0.77037446]]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals, evecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164.00000000000043, 164.00000000000006)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(evals), np.linalg.det(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37.0, 37)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(evals), np.trace(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-398.0000000000008, -398.0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals.prod(), np.linalg.det(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 40.99999999999996)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trace(A), np.sum(evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors if A is symmetrical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[681.28476795,   2.3989822 ,  -9.3305183 ],\n",
       "       [  2.3989822 , 678.87066629,   1.51813915],\n",
       "       [ -9.3305183 ,   1.51813915, 681.83669978]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Let's make a symmetrical matrix\n",
    "# A Matrix M is symmetrical iff M.T == M\n",
    "A  = np.random.randint(10, 100, size=(5000, 3))\n",
    "A_sym = np.cov(A.T) \n",
    "A_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([690.92441379, 671.2152533 , 679.85246694]),\n",
       " array([[-0.70101091, -0.68125763,  0.21088326],\n",
       "        [-0.04991863,  0.34185566,  0.93842572],\n",
       "        [ 0.71140132, -0.64731967,  0.27365199]]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals, evecs = np.linalg.eig(A_sym)\n",
    "evals, evecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### normal -> all eigenvectors are normal, always\n",
    "# A vector v is normal iff length of v is 1\n",
    "np.sqrt(np.sum(evecs[:, 0]**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.706168622523819e-15"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### because A_sym is symmetrical the vectors are also orthogonal\n",
    "# vectors a and b are orthogonal iff the angle between a and b is 90 degree (dot product = 0)\n",
    "\n",
    "np.dot(evecs[:, 0], evecs[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## if A*B = I -> A, B are inverses\n",
    "## evecs.T = evecs.inv()\n",
    "np.round(evecs.dot(evecs.T), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "### eigenvecs and vals redescribe your space \n",
    "### if your space is symmetrical, then the eigenvecs are a basis\n",
    "### so...why is this important for dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "1. Calculate the covariance matrix of your data, C -> Symmetrical\n",
    "2. Calculate the eigenvecs and eigenvalues of covariance matrix\n",
    "3. Project our data onto the vectors that most describe the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[681.28476795,   2.3989822 ,  -9.3305183 ],\n",
       "       [  2.3989822 , 678.87066629,   1.51813915],\n",
       "       [ -9.3305183 ,   1.51813915, 681.83669978]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 339.53151684,   22.8263153 ,  101.99325328],\n",
       "       [  23.49657348,   78.44176611, -412.98415117],\n",
       "       [ 103.65429583, -407.73737703,   50.91103213]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition for non square matrix\n",
    "- SVD (Singular Value Decomposition)\n",
    "- Reduces any matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment\n",
    "- I learned how information is being maintained as data is moved into lower dimensional space\n",
    "- I now understand that after (eigen decomposition) calculating eigens, we can project the data into four quadrants based upon eigen V1 and V2\n",
    "- The principal component.  Comes from the covariance matrix. The vector that explains the most variance and retains the most of the original data info\n",
    "- I definitely need a lot more time with this topic. I did learn that PCA is useful for finding relatedness between populations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
